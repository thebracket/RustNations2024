# Why isn't it a linear speedup?

So let's add a bit of `println` instrumentation to try and figure out why we aren't getting a linear performance improvement.

```rust
use std::sync::Arc;
use std::sync::Mutex;
use std::time::Instant;

// DELETE THIS:
//const NUM_THREADS: usize = 10;
const MAX_NUMBER: usize = 100_000;

/// Really inefficient prime number calculator
fn is_prime(n: usize) -> bool {
    if n <= 1 {
        false
    } else {
        for div in 2..n {
            if n % div == 0 {
                return false;
            }
        }
        true
    }
}

fn main() {
    let num_cpus = num_cpus::get();
    println!("Using {num_cpus} threads.");
    let candidates: Vec<usize> = (0..MAX_NUMBER).collect();

    // Perform the calculation
    let start = Instant::now(); // We're not timing the initial creation
    let primes: Arc<Mutex<Vec<usize>>> = Arc::new(Mutex::new(Vec::new()));

    std::thread::scope(|scope| {
        let chunks = candidates.chunks(candidates.len() / num_cpus);
        // We'll use enumerate() to get the index of each chunk
        for (id, chunk) in chunks.enumerate() {
            // And check the chunk sizes
            println!("Thread #{id} is using chunk size: {}", chunk.len());
            let my_primes = primes.clone();
            scope.spawn(move || {
                // And time each chunk
                let chunk_start = Instant::now();

                let local_results: Vec<usize> =
                    chunk.iter().filter(|n| is_prime(**n)).map(|n| *n).collect();
                let mut lock = my_primes.lock().unwrap();
                lock.extend(local_results);

                // Print the time for each chunk
                let chunk_elapsed = chunk_start.elapsed();
                println!("Thread #{id} took {:.4} seconds", chunk_elapsed.as_secs_f32());
            });
        }
    });
    let elapsed = start.elapsed();

    // Results
    let lock = primes.lock().unwrap();
    println!("Found {} primes", lock.len());
    //println!("{:?}", *lock);
    println!("Calculated in {:.4} seconds", elapsed.as_secs_f32());
}
```

The results are interesting:

First of all, thread chunk sizes are nice and even:

```
Using 20 threads.
Thread #0 is using chunk size: 5000
Thread #1 is using chunk size: 5000
Thread #2 is using chunk size: 5000
Thread #3 is using chunk size: 5000
Thread #4 is using chunk size: 5000
Thread #5 is using chunk size: 5000
Thread #6 is using chunk size: 5000
Thread #7 is using chunk size: 5000
Thread #8 is using chunk size: 5000
Thread #9 is using chunk size: 5000
Thread #10 is using chunk size: 5000
Thread #11 is using chunk size: 5000
Thread #12 is using chunk size: 5000
Thread #13 is using chunk size: 5000
Thread #14 is using chunk size: 5000
Thread #15 is using chunk size: 5000
Thread #16 is using chunk size: 5000
Thread #17 is using chunk size: 5000
Thread #18 is using chunk size: 5000
Thread #19 is using chunk size: 5000
```

But each chunk is taking gradually longer to execute (I sorted the results here for clarity):

```
Thread #0 took 0.0080 seconds
Thread #1 took 0.0149 seconds
Thread #2 took 0.0136 seconds
Thread #3 took 0.0271 seconds
Thread #4 took 0.0330 seconds
Thread #5 took 0.0381 seconds
Thread #6 took 0.0413 seconds
Thread #7 took 0.0344 seconds
Thread #8 took 0.0386 seconds
Thread #9 took 0.0412 seconds
Thread #10 took 0.0362 seconds
Thread #11 took 0.0446 seconds
Thread #12 took 0.0499 seconds
Thread #13 took 0.0539 seconds
Thread #14 took 0.0696 seconds
Thread #15 took 0.0627 seconds
Thread #16 took 0.0755 seconds
Thread #17 took 0.0701 seconds
Thread #18 took 0.0666 seconds
Thread #19 took 0.0662 seconds
```

Looking at our `is_prime` function, the larger the number being evaluated - the more loop iterations may be needed. So if our chunks are evenly divided into spans, early spans have a lot less to do. Let's check that:

```rust
// Add this after println!("Thread #{id} is using chunk size: {}", chunk.len());
println!("Thread #{id} starts at: {}", chunk[0]);
```

And as we suspected, each chunk is consequitive in the list. Now we put on our engineer hat for a minute and think about tradeoffs:

* Continuous ranges in spans make the chunk type a simple list of memory-adjacent numbers.
* CPU caches love continuous ranges, and the prefetcher will make reading the list of numbers
  really fast.
* So you can see why `chunks` behaves that way, and why you may not want to change it too much.
* On the other hand, we should get better total performance if the distribution of workload
  doesn't always mean waiting for the later thread to finish!
* So potentially we have a performance tradeoff to consider.
